<h2>Related Work</h2>

<p>
    The study of geometry in learning systems has roots in classical differential geometry texts, and has influenced modern perspectives on representation learning from early manifold learning frameworks such as ISOMAP
    <d-cite bibtex-key="tenenbaum2000global"></d-cite>, LLE <d-cite bibtex-key="roweis2000nonlinear"></d-cite>, and Laplacian Eigenmaps <d-cite bibtex-key="belkin2003laplacian"></d-cite>.
</p>
<p>
    In deep learning and interpretability, the idea that neural networks learn structured manifolds and sometimes nearly linear features has been explored in overviews and blog essays
    <d-cite bibtex-key="olah2014neural"></d-cite><d-cite bibtex-key="olah2024linear"></d-cite>, as well as in empirical studies on linear representations in large language models (LLMs)
    <d-cite bibtex-key="tigges2023linear"></d-cite><d-cite bibtex-key="jiang2024origins"></d-cite>.
</p>
<p>
    Recent work evaluates the extent to which features are linear
    <d-cite bibtex-key="engels2024not"></d-cite>, proposes the Linear Representation Hypothesis
    <d-cite bibtex-key="park2024linear"></d-cite> and its broader geometric context
    <d-cite bibtex-key="park2024geometry"></d-cite>, and investigates counterpoints where token embeddings may violate manifold assumptions
    <d-cite bibtex-key="robinson2024token"></d-cite>.
</p>
<p>
    Broader empirical phenomena like grokking <d-cite bibtex-key="power2022grokking"></d-cite> and mechanistic case studies using activation patching
    <d-cite bibtex-key="heimersheim2024activation"></d-cite>, subspace methods and cautions
    <d-cite bibtex-key="makelov2023subspace"></d-cite>, and nullspace-projection techniques for controlling protected attributes
    <d-cite bibtex-key="ravfogel2020null"></d-cite> relate to the geometry and linearity of learned features.
</p>
<p>
    Interpretability illusions and generalization challenges have been noted
    <d-cite bibtex-key="friedman2024interpretability"></d-cite><d-cite bibtex-key="hase2023localization"></d-cite>, while complementary analyses of weight matrices via SVD
    <d-cite bibtex-key="millidge2022singular"></d-cite> and discussions of near-orthogonality in high dimensions connect theory and intuition
    <d-cite bibtex-key="johnson1984extensions"></d-cite><d-cite bibtex-key="3blue1brown2024high"></d-cite><d-cite bibtex-key="yoder2025johnson"></d-cite>.
</p>
<p>
    Finally, several works highlight non-linear structure and stratification in learned representations
    <d-cite bibtex-key="li2025unraveling"></d-cite>, as well as emerging directions in numerical reasoning geometry
    <d-cite bibtex-key="numerical2024geometry"></d-cite> and non-linear probing
    <d-cite bibtex-key="white2023nonlinear"></d-cite>, complementing trend reports and community syntheses
    <d-cite bibtex-key="lesswrong2024intricacies"></d-cite><d-cite bibtex-key="lampinen2024learned"></d-cite>.
</p>
