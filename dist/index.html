<!DOCTYPE html>
<html>

<head>
    <script src="distill.bundle.js" type="module" fetchpriority="high" blocking></script>
    <script src="main.bundle.js" type="module" fetchpriority="low" defer></script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf8">
    <base target="_blank">
    <title>The Ultra-Scale Playbook:¬†Training LLMs on GPU Clusters</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <d-front-matter>
        <script id='distill-front-matter' type="text/json">{
    "title": "The Ultra-Scale Playbook:¬†Training LLMs on GPU Clusters",
    "description": "This blog covers everything about scaling LLMs in 2025.",
    "published": "Sept 28, 2024",
    "affiliation": {"name": "HuggingFace"},
    "authors": [
      {
        "author":"Leandro Werra",
        "authorURL":"https://huggingface.co/lvwerra"
      },
      {
        "author":"Thomas Wolf",
        "authorURL":"https://huggingface.co/thomwolf"
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }
    </script>
    </d-front-matter>
    <d-title>
        <h1 class="l-page" style="text-align: center;">The Ultra-Scale Playbook:¬†Training LLMs on GPU Clusters</h1>
        <div id="title-plot" class="main-plot-container l-screen">
            <figure>
                <img src="assets/images/banner.png" alt="FineWeb">
            </figure>
            <!-- <div id="clusters-plot">
            <img src="assets/images/clusters.png" alt="Clusters">
        </div> -->
        </div>
    </d-title>
    <d-byline></d-byline>
    <d-article>
        <d-contents>
        </d-contents>
        
        <p>Fueled by the scaling laws<d-cite bibtex-key="kaplan2020scalinglaws"></d-cite><d-cite bibtex-key="hoffmann2022chinchilla"></d-cite>, the trend of training ever larger language models on vaster amounts of data has been driving progress in AI for the past couple years. Initially, the development of the largest models happened exclusively behind closed doors of a handful of research labs but recently opened up more with the release of models such as Llama 3.1 405B<d-cite bibtex-key="grattafiori2024llama3herdmodels"></d-cite> and DeepSeek R1<d-cite bibtex-key="deepseekai2024deepseekv3technicalreport"></d-cite>. While these models have <a href="https://huggingface.co/meta-llama">openly shared</a> <a href="https://huggingface.co/deepseek-ai">weights</a> and their training recipes are described in <a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">technical</a> <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">reports</a>, the challenging engineering to involved to train at the necessary infrastructure scale is still hidden between the lines of a handful of papers and complex training frameworks. This ~~long blog post~~ open-source book is here to open this black box!</p>

        <aside>Reading time: 7 days. For the best reading experience, we recommend not using a mobile phone.</aside>

        <p>In this book we invite you to follow us in the wonderful world of scaling training of Large Language Models to tens, hundreds, thousands of GPUs. It assumes you know the basics on LLM architecture and training, but are new to distributed training. This writing can be seen as a second part of a trilogy following our first blog on processing data for pre-training, the so-called ‚Äú<a href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb blog post</a>‚Äù. Having read both blog posts, you should have almost all the core knowledge needed to deeply understand how LLMs are being built nowadays, just missing a bit the final spices like data mixing or architecture choices to complete the recipe (stay tuned‚Ä¶).</p>

        <p>Pre-training LLMs from scratch now requires amounts of compute which exceed in almost every case the use of a single GPU or machine. The clusters used to train these models range from hundreds to thousands of nodes each usually equipped with 4 to 8 GPUs. To make the best use of such an expensive hardware as well as to train in a reasonable time, a range of distributed training methods have been developed with the goal of ensuring that GPUs are highly utilized at all times. Efficiently scaling LLM training is also not confined to pretraining anymore, as fine-tuning larger models on more domain specific data is becoming the standard practice to achieve the best results.</p>

        <aside>We are extremely thankful to the whole <a href="https://distill.pub/">distill.pub</a> team for creating
            the template on which we based this blog post.</aside>
        
        <p>In this post we‚Äôll cover these scaling methods exhaustively while keeping a single story-line to understand where each technique comes from. We‚Äôll cover data, tensor, pipeline and context parallelism as well as ZeRO and kernel fusion. The post is built on the following <strong>three foundations</strong>:</p>

        <p><strong>Quick intros on theory and concepts:</strong> before diving into code and experiments, we want to understand how each method works at a high level and what it‚Äôs advantages and limits are. You‚Äôll learn about which parts of a language model eat away your memory and when during training it happens. You‚Äôll learn how we can solve memory constraints by parallelizing the models and increase the throughput by scaling up GPUs. As a result you'll understand how the following widget to compute the memory breakdown of a transformer model works: </p>

        <div id="graph"></div>
        <div id="controls">
            <div class="cell column-1">
            <label for="a">Attention Heads (a):</label>
            <input type="range" id="a" name="a" min="1" max="128" value="8">
            <input type="number" id="a_input" value="8" min="1" max="128">
            </div>
            <div class="cell column-2">
            <label for="mixed">Mixed Precision:</label>
            <input type="checkbox" id="mixed" name="mixed" checked>
            <span></span> <!-- Empty span to maintain grid alignment -->
            </div>
            <div class="cell column-1">
            <label for="b">Micro Batch Size (b):</label>
            <input type="range" id="b" name="b" min="1" max="53248" value="32">
            <input type="number" id="b_input" value="32" min="1" max="53248">
            </div>
            <div class="cell column-2">
            <label for="seq_parallel">Sequence Parallelism:</label>
            <input type="checkbox" id="seq_parallel" name="seq_parallel">
            <span></span> <!-- Empty span to maintain grid alignment -->
            </div>
            <div class="cell column-1">
            <label for="h">Hidden Dimension (h):</label>
            <input type="range" id="h" name="h" min="1" max="16384" value="512">
            <input type="number" id="h_input" value="512" min="128" max="16384">
            </div>
            <div class="cell column-2">
            <label for="recomputation">Recomputation:</label>
            <select id="recomputation" name="recomputation">
                <option value="none">None</option>
                <option value="selective">Selective</option>
                <option value="full">Full</option>
            </select>
            <span></span> <!-- Empty span to maintain grid alignment -->
            </div>
            <div class="cell column-1">
            <label for="h_ff">Feedforward Dimension (h_ff):</label>
            <input type="range" id="h_ff" name="h_ff" min="1" max="65536" value="2048">
            <input type="number" id="h_ff_input" value="2048" min="512" max="65536">
            </div>
            <div class="cell column-2">
            <label for="zero">Zero:</label>
            <select id="zero" name="zero">
                <option value="0">0</option>
                <option value="1">1</option>
                <option value="2">2</option>
                <option value="3">3</option>
            </select>
            <span></span> <!-- Empty span to maintain grid alignment -->
            </div>
            <div class="cell column-1">
            <label for="L">Number of Layers (L):</label>
            <input type="range" id="L" name="L" min="1" max="126" value="12">
            <input type="number" id="L_input" value="12" min="1" max="126">
            </div>
            <div class="cell column-2">
            <label for="ff_activation">FF Activation:</label>
            <select id="ff_activation" name="ff_activation">
                <option value="relu">ReLU</option>
                <option value="gelu">GELU</option>
                <option value="swiglu">SwiGLU</option>
            </select>
            <span></span> <!-- Empty span to maintain grid alignment -->
            </div>
            <div class="cell column-1">
            <label for="s">Sequence Length (s):</label>
            <input type="range" id="s" name="s" min="1" max="128000" value="128">
            <input type="number" id="s_input" value="128" min="64" max="128000">
            </div>
            <div class="cell column-2">
            <label for="presets">Presets:</label>
            <select id="presets" name="presets">
                <option value="Llama 3 Tiny">Llama 3 Tiny</option>
                <option value="Llama 3 8B">Llama 3 8B</option>
                <option value="Llama 3 70B">Llama 3 70B</option>
                <option value="Llama 3 405B">Llama 3 405B</option>
            </select>
            <span></span> <!-- Empty span to maintain grid alignment -->
            </div>
            <div class="cell column-1">
            <label for="v">Vocabulary Size (v):</label>
            <input type="range" id="v" name="v" min="1000" max="100000" value="30522">
            <input type="number" id="v_input" value="30522" min="1000" max="100000">
            </div>
            <div class="cell column-2">
            <label for="tp">Tensor Parallelism (t):</label>
            <input type="range" id="tp" name="tp" min="1" max="16" value="8">
            <input type="number" id="tp_input" value="8" min="1" max="16">
            </div>
            <div class="cell column-1">
            <label for="k">Optimizer Parameters (k):</label>
            <input type="range" id="k" name="k" min="1" max="16" value="8">
            <input type="number" id="k_input" value="8" min="1" max="16">
            </div>
            <div class="cell column-2">
            <label for="dp">Data Parallelism (d):</label>
            <input type="range" id="dp" name="dp" min="1" max="256" value="1">
            <input type="number" id="dp_input" value="1" min="1" max="256">
            </div>
        </div>

        <p>While this widget gives a theoretical breakdown the following tool can be used to predict the memory usage:</p>
        
        <p><img alt="image.png" src="assets/images/placeholder.png"/></p>

        <p><strong>Clear code implementations:</strong> theory is one thing, but we discover all kinds of edge cases and important details when we implement something. That‚Äôs why we link to implementation references where possible. Depending on the case, we‚Äôll use two code references: the <a href="https://github.com/huggingface/picotron">picotron</a> repository is built for education, thus it implements concepts usually in single, self-contained short files. On the other hand, to look at production ready code, we‚Äôll refer to the <a href="https://github.com/huggingface/nanotron">nanotron</a> implementations which is a production training codebase used at Hugging Face.</p>

        <p><img alt="Picotron implements each key concept in a self-contained way, such that the method can be studied separately and in isolation." src="assets/images/placeholder.png" /></p>

        <p><strong>Real training efficiency benchmarks:</strong> Finally, how to <em>actually</em> scale your LLM training depends on your infrastructure, such as the kind of chips, interconnect etc., and we can‚Äôt give a single unified recipe. What we will give though is a way to benchmark several setups and it is what we have done on our cluster! We ran over 4100 distributed experiments with up to 512 GPUs to scan many possible distributed training layouts and model sizes. TODO: link to dataset too </p>

        <p><img alt="An overview of the over 4000 experiments across all Llama architectures where each data point corresponds to an experiment launch." src="assets/images/placeholder.png" /></p>

        <p>As you can see, there‚Äôs a lot of ground to be covered. Before getting into the trenches of distributed training let‚Äôs take a quick high level look on we‚Äôll cover in the post.</p>

        <h2>TL;DR</h2>

        <p>This book is very extensive so we decide to start with a very general overview of how you can think about distributed training. At a high level, the key challenge in scaling LLM training is to make a training step (forward/backward/optimizer step) with a large batch size the fastest possible.</p>
        <p>When scaling up models and input batches, we quickly end up in situations where either our target batch size won't fit in memory, or/and the model itself is too large to fit in a single GPU's memory.</p>
        <p>To solve this scaling issue we‚Äôll need to carefully evaluate different parallelization strategies and find the optimal balance between three main factors:</p>
        <ol>
        <li><strong>Memory Usage</strong><ul>
        <li>Hard limitation - if a training step doesn't fit in memory, training cannot proceed</li>
        <li>Sometimes we can increase compute (e.g. recomputation) or increase communication (e.g. ZeRO) to reduce memory usage</li>
        </ul>
        </li>
        <li><strong>Compute Efficiency</strong><ul>
        <li>Memory transfer can also decrease compute efficiency.</li>
        <li>We want our hardware to spend most time computing, so we need to reduce time spent on data transfers or unoptimized kernels.</li>
        <li>GPUs need sufficient workload (large enough matrices/batch sizes) to maintain high utilization (compute-bound) otherwise they become memory-bound (limited by memory bandwidth).</li>
        </ul>
        </li>
        <li><strong>Communication overhead</strong><ul>
        <li>Two main types. For GPUs: intra-node (NVLink TODO: bandwidth) and inter-node (network TODO: bandwidth)</li>
        <li>Two main attributes: base latency and peak bandwidth. Base latency is a constant overhead that makes us want to do the least number of comms possible, and peak bandwidth controls the how fast we can move data between gpus</li>
        <li>We prioritize using the fastest communication channels (like NVLink) for operations that occur frequently and/or block computation (e.g. tensor parallelism)</li>
        <li>We want to minimize communication overhead as it keeps GPUs idle, so we try to overlap communication with compute as much as possible</li>
        </ul>
        </li>
        </ol>
        <p>But let‚Äôs not get too much ahead of our self and scale progressively. To guide you along the journey and as a practical reference we summarized the key concepts in a cheatsheet:</p>
        <p>[TODO: ADD CHEATSHEET]</p>
        <p>Now that we nailed a few key concept and terms let‚Äôs get started by revisiting the basic training steps of an LLM!</p>
                
        <h2>First Steps: Training on one GPU</h2>
        
        <p>Let‚Äôs start by quickly reviewing the very basics of model training before we start to scale to many GPUs. When a model is trained on a single GPU, the training typically consists of three steps: </p>

        <ol>
            <li>a forward pass which passes inputs through the model to yield its outputs,</li>
            <li>a backward pass to compute the gradients, and</li>
            <li>an optimization step using the gradients to update the parameters</li>
        </ol>

        <p>It looks generally like this: </p>
        <p><img alt="image.png" src="assets/images/placeholder.png" /></p>

        <aside>As we‚Äôll see later, these steps may be repeated or intertwined but for now we‚Äôll start simple.</aside>

        <p>In this figure, the boxes on the top line can be seen as successive layers inside a model (same for the last line). The red boxes are the associated gradients for each of these layers, computed during the backward pass.</p>

        <p>The batch size (<d-math>bs</d-math>) is one of the important hyper-parameters for model training and affects both model convergence and throughput.</p>

        <p>If the batch size is too small, gradients will tend to be noisy and the model may not be able to converge to the most optimal performance, on the contrary it can be useful in early training to navigate quickly in the training landscape. On the other hand, a batch size too large will make less use of each training token rendering convergence slower and wasting compute. You can find a nice discussion of this topic in OpenAI‚Äôs paper on large batch training<d-cite bibtex-key="mccandlish2018largebatchtraining"></d-cite> or Section 4.2 of MiniMax-01 <a href="https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf">technical report</a>.</p>

        <aside>For instance, during DeepSeek-V3/R1 training ‚Äúthe batch size is gradually increased from 3072 to 15360 in the training of the first 469B tokens, and then keeps 15360 in the remaining training‚Äù.</aside>

        <p>Batch size also affects the time it takes to train on a given text dataset: a small batch size will require more optimizer steps to train on the same amount of samples. Optimizer steps are costly (in compute time) and the total time to train will thus increase compared to a larger batch size. This being said, note that the batch size can often be adjusted quite largely around the optimal batch size without major impact to the performance of the model, i.e. the sensitivity of final model performances to the exact batch size value is usually rather low around the optimal batch size.</p>

        <p>In the LLM pretraining community, batch sizes are commonly reported in terms of tokens rather than in number of samples (<d-math>bst</d-math> = Batch Size Tokens), this makes training numbers generally independent of the exact input sequence length used during the training.</p>

        <p>In the simplest case, training on a single machine, the <d-math>bs</d-math> (in samples) and <d-math>bst</d-math> can be computed from the model input sequence length (seq) as follows :</p>

        <aside><p>From here onward we‚Äôll show the formulas for the batch size in terms of samples but you can always get its token-unit counterpart by multiplying it with the sequence length.
        </aside>

        <d-math block>
        bst=bs *seq
        </d-math>

        <p>A sweet spot for recent LLM training is typically on the order of 4-60 million tokens per batch. However, a typical issue when scaling the training of our model to these large batch sizes is out-of-memory issues, ie. our GPU doesn‚Äôt have enough memory.</p>

        <aside>Note: Llama 1 was trained with a batch size of ~4M tokens for 1.4 trillions tokens while DeepSeek was trained with a batch size of ~60M tokens for 14 trillion tokens.
        </aside>

        <p><strong>It‚Äôs time to tackle our first scaling problem: what if our model starts exploding GPU memory before we‚Äôve reached our target batch size (maybe in some case even when using the lowest possible batch size, <code>bs=1</code>)?</strong></p>

        <p>Let‚Äôs start by quickly understanding what led to our out-of-memory issue in the first place. This will help us gain some useful intuitions for later.</p>

        <h3>Memory usage in Transformers</h3>

        <p>When training a neural network model, one store several items in memory:</p>

        <ul>
            <li>Model weights</li>
            <li>Activations needed to compute the gradients</li>
            <li>Model gradients</li>
            <li>Optimizer states</li>
        </ul>
        
        <aside >You would think for a model you could compute the memory requirements exactly but there are a few additional memory occupants that makes it hard to be exact:
        <ul>
            <li>CUDA Kernels typically require 1-2 GB of GPU memory, which you can quickly verify by running <code>import torch; torch.ones((1, 1)).to("cuda")</code> and then checking the GPU memory with <code>nvidia-smi</code>.</li>
            <li>Some rest memory usage from buffers, intermediate results and some memory that can‚Äôt be used due to fragmentation</li>
        </ul>
        We‚Äôll neglect these last two contributors as they are typically small and constant factors.</aside>

        <p>These items are stored as tensors which come in different <em>shapes</em> and <em>precisions</em>. The <em>shapes</em> are determined by hyper-parameters such as batch size, sequence length, model hidden dimensions, attention heads, vocabulary size, and potential model sharding as we‚Äôll see later. <em>Precision</em> refers to formats like FP32, BF16, or FP8, which respectively require 4, 2, or 1 byte to store each single value in the tensor.</p>

        <p>So how can I quickly determine memory usage from these variable? One simple way is to do this empirically and just measure it.</p>

        <h4>Memory profiling a training step</h4>

        <p>Using this snippet [TODO: link to appendix A5], we can understand how memory is allocated throughout training. We can see that memory utilization is not a static thing but varies a lot during training and during a training step:</p>

        <p><img alt="llama-1b-memory.png" src="assets/images/placeholder.png" /></p>

        <p>Clearly the first step looks very different from the subsequent ones, but let‚Äôs first have a look at the general anatomy of a step: first the activations increase quickly as we do the forward pass, then during the backward pass the gradients build up and as the backward pass propagates, the stored activations used to compute the gradients are progressively cleared. Finally, we perform the optimization step during which we need all the gradients and then update the optimizer states before we start the next forward pass. </p>

        <p>Why does the first step looks different: the activations increase quickly and then plateau for a while. In this first step the torch cache allocator does a lot of preparation preparing memory allocations to speed up the subsequent steps so that they don‚Äôt require searching for free memory blocks afterwards (see <a href="https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html">Zach‚Äôs blog</a>). After the first step we also see the optimizer states appearing which generally offset the memory usage for further training steps.</p>

        <aside>Ever noticed how sometimes the training succeeds in the first step but then OOMs during the following training steps? This can be explained by the build-up of the optimizer state after the first step.
        </aside>

        <p>Now that we‚Äôve a first view of memory, let‚Äôs see how scaling up training is often a question of maximizing compute efficiency while keeping the memory requirements of these various items (activations, parameters, gradients, optimizer states) within the memory constraints of the GPUs.</p>

        <h4>Weights/grads/optimizer states memory</h4>

        <p>We can actually pretty easily estimate the memory needed for the model‚Äôs weights, gradients and optimizer states.</p>

        <p>For a simple transformer LLM the number of parameters is given by the <a href="https://michaelwornow.net/2024/01/18/counting-params-in-transformer">following formula</a>:</p>

        <d-math block>
            N = h * v + L * (12 * h^2 + 13 * h) + 2*h
        </d-math>

        <aside>We excluded the positional embedding count as rotary embeddings are not learned.</aside>

        <p>In that equation, <d-math>h</d-math> is the hidden dimension, <d-math>v</d-math> the vocabulary size, and <d-math>L</d-math> the number of layers in the model. Note that looking at the equation we can see that the term that will dominate at large hidden dimensions is the <d-math>h^2</d-math> term since it‚Äôs the only one growing quadratically as we scale the parameters.</p>

        <p>Memory requirements for the parameters and gradients are simply the number of parameters multiplied by the number of bytes per parameter. In good old-fashioned full precision (FP32) training both parameters and gradients require 4 bytes while the optimizer, if we use Adam, requires the momentum and variance to be stored, which adds another two 4 bytes per parameter. In summary:</p>

        <d-math block>
            \begin{aligned}
            & m_{params} = 4 * N \\
            & m_{grad} = 4 * N \\
            & m_{opt} = (4+4) * N
            \end{aligned}
        </d-math>

        <p>Now let‚Äôs have look how things change if we train with mixed precision<d-cite bibtex-key="micikevicius2018mixedprecisiontraining"></d-cite>. The default nowadays is for mixed precision training is BF16, requires 2 bytes per parameter and gradient as well as an additional copy of the model weights and gradients in FP32, thus 12 bytes per parameter in total. In addition to the parameters and gradient, we need to store the optimizer states: for the Adam optimizer, this requires the momentum and the variance usually stored in FP32 for numerical stability, each using 4 bytes. </p>

        <aside>See some more details below when we cover the ZeRO methods.</aside>

        <p>Here‚Äôs the summary:</p>
        
        <d-math block>
            \begin{aligned}
                & m_{params} = 2 * N \\ 
                & m_{grad} = 2 * N \\ 
                & m_{params_fp32} = 4 * N  \\ 
                & m_{opt} = (4+4) * N
            \end{aligned}
        </d-math>

        <aside>Some librarie store grads in fp32 which would require an additional $m_{params_fp32} = 4 * N$ memory. This is done for example in nanotron, because <code>bf16</code> is lossy for smaller values and we always prioritize stability. See  <a href="https://github.com/microsoft/DeepSpeed/issues/1773">this DeepSpeed issue</a> for more information.
        </aside>

        <p>Interestingly, mixed precision itself doesn‚Äôt save overall memory as it just distributes the memory differently across the three components, and in fact adds another 4 bytes over full precision training if we accumulate gradients in FP32. It‚Äôs still advantageous as having the model which does the forward/backward in half precision it allows us to (1) use optimized lower precision operations on the GPU which are faster and (2) reduces the activation memory requirements during the forward pass.</p>

        <p>Let‚Äôs get a sense of how much general memory we need for a model (full and mixed precision giving the same overall value):</p>

        <table>
        <thead>
            <tr>
            <th><strong>Model parameters</strong></th>
            <th><strong>FP32 or BF16 w/o FP32 grad acc</strong></th>
            <th><strong>BF16 w/ FP32 grad acc</strong></th>
            </tr>
        </thead>
        <tbody>
            <tr>
            <td>1B</td>
            <td>16 GB</td>
            <td>20 GB</td>
            </tr>
            <tr>
            <td>7B</td>
            <td>112 GB</td>
            <td>140 GB</td>
            </tr>
            <tr>
            <td>70B</td>
            <td>1120 GB</td>
            <td>1400 GB</td>
            </tr>
            <tr>
            <td>405B</td>
            <td>6480 GB</td>
            <td>8100 GB</td>
            </tr>
        </tbody>
        </table>

        <aside><p>Using FP8 training instead of BF16 would further decrease the memory usage but it is less stable and a very active research topic (see <a href="https://x.com/xariusrke/status/1826669126955278401">this tweet</a>) and we‚Äôll cover it in more detail later.
        </aside>

        <p>As we can see, as soon as we reach <strong>7B</strong> (!), weights and optimizer requirements already starts to add up significantly and exceed the size of a typical GPU memory, e.g. 80GB for a H100 GPU.</p>
        
        <p>But for now, let‚Äôs start with models which still fits in a single GPU, take a look at the other big contributor to our memory budget: the activation memory.</p>

        <h4>Activations memory</h4>
        
        <p>Activation memory is a bit more complex to compute than the weights, gradients and optimizer states, in part because it depends on the inputs of the model. If you‚Äôre unsure why we even need to store activations for the backward pass, <a href="https://www.determined.ai/blog/act-mem-2">this reference</a> is a good quick refresh. After a careful inspection of how backward pass is computed we can estimate the total memory required for the activations in mixed precision and we arrive at the following equation:</p>

        <d-math block>
            m_{act} =  L<em> seq * bs * h * (34 + \frac{5</em>n_{heads}*seq}{h})</p>
        </d-math>

        <p>Here <d-math>L</d-math> is the number of layers, <d-math>seq</d-math> the sequence length, <d-math>bs</d-math> the batch size in samples, <d-math>h</d-math> the hidden dimension of the model and <d-math>n_{heads}</d-math> the number of heads.</p>

        <p>For the exact derivation of the numbers, you can follow this original NVIDIA paper on recomputation <d-cite bibtex-key="korthikanti2022recomputation"></d-cite>, it essentially requires you to do some accounting of all the sizes of intermediate activations between each operation in a transformer layer.</p>

        <p>An interesting observation here is how the memory is not static for a given model but it scales linearly with both the sequence length and batch size. This means the activation memory is the part which will blow up when we increase our batch size or train with longer sequences. We can use this equation to look at how memory usage changes for various sequence lengths for example for Llama models (<code>bs=1</code>):</p>

        <p><img alt="llama-memory-bars-no-recomp.png" src="/assets/images/placeholder.png" /></p>

        <p>This graph tells a striking story: for short sequences (or similar for small batch-sizes), activations are almost negligible, but starting at around 2-4k tokens they come to take a significant amount of memory while parameter, gradient and optimizer states usage (that we‚Äôll discuss later) stays roughly independent of the sequence length and batch size.</p>

        <p><strong>For large input tokens (a.k.a large batch-sizes/sequences), activations become by far the largest memory burden.</strong> </p>

        <p>Is there a way to tame this ‚Äúactivation explosion‚Äù? Good question, reader!</p>

        <p>It‚Äôs time to explain our first technique ‚Äì called <strong><em>activation recomputation</em><em>‚Äì</em> </strong>**which will help us cap activation memory footprint. An essential tool in today‚Äôs large model training toolbox.</p>

        <h3>Activation recomputation</h3>
        
        <p>The general idea behind <strong><em>activation recomputation</em></strong> ‚Äì also called <em>gradient checkpointing</em> or <em>rematerialization</em> ‚Äì is to discard some activations during the forward pass to save memory and spend some extra compute to recompute these on the fly during the backward pass. Without recomputation, we store every hidden state between two learnable operations (e.g. FF, LayerNorm etc.), such that we can use them during the backward pass to compute gradients. When we use recomputation we typically will only store activations at a few key points along the model architecture, discard the rest of activations and recompute them on the fly during the backward pass from the nearest saved activations, basically performing again a sub-part of the forward pass to trade of memory for compute. It generally looks like this:</p>

        <p><img alt="image.png" src="/assets/images/placeholder.png" /></p>

        <p>There are several strategies to select key activations to store:</p>

        <ul>
            <li><strong>Full</strong>: We checkpoint activations at the transition point between each layer of the Transformer model. This is usually called the <code>full</code> strategy since it requires a forward pass through each layer essentially adding a full forward pass during the backward pass. This strategy saves the most memory but is the most expensive one in terms of compute. It generally increases the compute cost and time by up to 30-40% which is very noticeable.</li>
            <li><strong>Selective</strong>: In general we can do better than full. The authors of the recomputation paper<d-cite bibtex-key="korthikanti2022recomputation"></d-cite> did a detailed analysis studying which activations grow the largest and have the cheapest recomputation cost in terms of FLOPs. Turns out that the attention computations fall in that category, and thus we can usually discard them and focus on checkpointing expensive the feedforward computations. For a GPT-3 (175B) model this means <strong>70% activation memory reduction at a 2.7% compute cost</strong>.</li>
        </ul>

        <aside>In recent models like DeepSeek V3, selective checkpointing is performed, storing even a smaller size of attention activation ‚Äîusing so-called ‚ÄúMulti-Head Latent Attention‚Äù (MLA)‚Äì to optimize activation memory usage.</aside>

        <p>Let‚Äôs see how drastically recomputation strategies can in practice reduce the memory footprint and how selective recomputation strikes a nice balance between memory saving and recomputation cost:</p>

        <p><img alt="llama-8b-memory-bars--recomp.png" src="/assets/images/placeholder.png" /></p>

        <aside>When you‚Äôre measuring how efficient your training setup is at using the accelerator‚Äôs available compute, you may want to take recomputation into account when measuring the total FLOPS (Floating point operations per second) of your training setup and comparing it to theoretical maximum FLOPS of your GPU/TPU/accelerator to estimate GPU utilization. Taking recomputation into account when calculating FLOPS for a training step gives a value called ‚Äúhardware FLOPS‚Äù which is the real number of operations performed on the accelerator. Dividing this number by the duration of one training step and the maximum accelerator FLOPS yields the <em>Hardware FLOPS Utilization (HFU).</em> </aside>
        
        <aside>However, when comparing various accelerators together, what really matters at the end of the day is the start-to-end time needed to train the same models on the same dataset, ie. if an accelerator allows to skip recomputation and thus perform less operation per second for a faster training it should be rewarded. Thus, alternative is to compute what is called <em>Model FLOPS Utilization (MFU)</em>, which in contrast to HFU only accounts for the required operations to compute the forward+backward passes, and not recomputation, ie. is specific to the model, not the training implementation.</aside>

        <p>Most training frameworks these days use FlashAttention (which we‚Äôll cover a bit later) which integrate natively activation recomputation in its optimization strategy by recomputing attention scores and matrices in the backward pass instead of storing them. Thus most people using Flash Attention are already making use of selective recomputation.</p>

        <p><strong>As you‚Äôve now understood, activation recomputation increases the number of FLOPs slightly due to recomputation, while it significantly reduces memory access overhead.</strong> </p>

        <p>This trade-off is particularly advantageous on hardware with small high-speed memory, like GPUs, as accessing memory is typically slower than performing computations. Despite the additional operations involves, the overall effect is thus often faster computation as well, in addition to the much lower memory footprint.</p>

        <p>Now that we‚Äôve learned about recomputation, we can tame the activations memory usage as we saw in the above graphs!</p>

        <p>However, activations still bears a linear dependance on the batch size and all our profiles in the barplots above were using <code>bs=1</code> so as we move to larger batch sizes it might become an issue again. Do not despair as we have a second tool in our box - <strong><em>gradient accumulation</em></strong> to the rescue!</p>

        <h3>Gradient accumulation</h3>

        <p>Now that we‚Äôve used activation recomputation to fit our model with a small batch size on a single GPU, we still need to reach our target batch size, let‚Äôs say 1M tokens (see our earlier discussion on optimal batch size). Gradient accumulation is a very straightforward method to avoid memory explosion when doing this.</p>

        <p>With <em>gradient accumulation</em> we split our batch into micro-batches, do forward and backward passes repeatedly on each micro-batch, compute the gradients, and, as the name suggests, sum the gradients for each micro-batch before doing a final optimizer step. In practice, we perform the optimization step not on the sum but on the average of the gradients, so the result is independent of the number of gradient accumulation steps.</p>

        <p>Let‚Äôs call the batch size for each forward pass the <code>micro batch size</code> (mbs). We‚Äôll refer to the overall batch size between each optimizer step as the <code>global batch size</code> (gbs). If we do one optimizer step for each 8 forward/backward passes, the <code>global batch size</code> will be 8 times the <code>micro batch size</code>.</p>

        <p>What we now call <code>global batch size</code> thus corresponds to what we‚Äôve called up to now just <code>batch size</code> for simplicity (we now make our terms more precise to avoid ambiguity).</p>

        <p>With gradient accumulation the global batch size can be simply computed as follows:</p>

        <d-math block>
            bs = gbs = mbs \times grad\_acc 
        </d-math>

        <p>Gradient accumulation allows us to effectively increase our batch size up to infinity (and beyond!) while the memory footprint stays constant. Gradient accumulation is also compatible with activation recomputation for further memory reduction. One drawback however, is that gradient accumulation requires multiple consecutive forward/backward passes per optimization step thereby increasing the compute overhead and slowing down training. No free lunch! </p>

        <p><img alt="image.png" src="/assets/images/placeholder.png" /></p>

        <p><strong>Gradient accumulation allows us to reduce memory of activations which grow linearly with batch size by computing only only partial, micro-batches. There is a small overhead caused by the additional forward and backward passes.</strong></p>

        <aside>Using gradient accumulation means we need to keep buffers where we accumulate gradients which persist throughout a training step. Whereas without gradient accumulation, in the backward gradients are computed while freeing the activations memory, which means a lower peak memory.</aside>

        <p>But if you‚Äôve carefully followed, you probably noticed that the forward/backward passes for each micro-batch can actually be run in parallel. Forward/backward passes are independent from each other, with independent input samples being the only difference. Seems like it‚Äôs time to start extending our training to more than one GPU! </p>

        <p>Let‚Äôs get a larger workstation üñ•Ô∏è  with a couple of GPUs and start investigating our first scaling technique called <em><strong>data parallelism</strong> which is just a parallel version of gradient accumulation</em>.</p>

        <p><strong>TODO: add profiling here or not?</strong></p>

        <h2>Data Parallelism</h2>
        
        <h4><strong>First optimization:</strong> Overlap gradient synchronization with backward pass</h4>
        
        <h4><strong>Second optimization:</strong> Bucketing gradients</h4>
        
        <h4><strong>Third optimization: I</strong>nterplay with gradient accumulation</h4>
        
        <h3>Revisit global batch size</h3>
        
        <h3>Our journey up to now</h3>
        
        <h3>ZeRO (<strong>Ze</strong>ro <strong>R</strong>edundancy <strong>O</strong>ptimizer)</h3>
        
        <h4>Memory usage revisited</h4>
        
        <h4>ZeRO-1: Partitioning Optimizer States</h4>
        
        <h4>ZeRO-2: Adding <strong>Gradient Partitioning</strong></h4>
        
        <h4>ZeRO-3: Adding <strong>Parameter Partitioning</strong></h4>
        
        <h2>Tensor Parallelism</h2>
        
        <h3>Tensor Parallelism in a Transformer Block</h3>
        
        <h3>Sequence Parallelism</h3>
        
        <h2>Context Parallelism</h2>
        
        <h3>Introducing Context Parallelism</h3>
        
        <h3>Discovering Ring Attention</h3>
        
        <h3>Zig-Zag Ring Attention ‚Äì A Balanced Compute Implementation</h3>
        
        <h2>Pipeline Parallelism</h2>
        
        <h3>Splitting layers on various nodes - All forward, all backward</h3>
        
        <h3>One-forward-one-backward and LLama 3.1 schemes</h3>
        
        <h3>Interleaving stages</h3>
        
        <h3>Zero Bubble and DualPipe</h3>
        
        <h2>Expert parallelism</h2>
        
        <h2>5D parallelism in a nutshell</h2>
        
        <h2>How to Find the Best Training Configuration</h2>
        
        <h2>Diving in the GPUs ‚Äì fusing, threading, mixing</h2>
        
        <h4>A primer on GPU</h4>
        
        <h3>How to improve performance with Kernels ?</h3>
        
        <h4>Memory Coalescing</h4>
        
        <h4>Tiling</h4>
        
        <h4>Thread Coarsening</h4>
        
        <h4>Minimizing Control Divergence</h4>
        
        <h3>Flash Attention 1-3</h3>
        
        <h3>Fused Kernels</h3>
        
        <h3>Mixed Precision Training</h3>
        
        <h4>FP16 and BF16 training</h4>
        
        <h4>FP8 pretraining</h4>
        
        <h2>Conclusion</h2>
        
        <h3>What you learned</h3>
        
        <h3>What we learned</h3>
        
        <h3>What‚Äôs next?</h3>
        
        <h2>References</h2>
        
        <h3>Landmark LLM Scaling Papers</h3>
        
        <h3>Training Frameworks</h3>
        
        <h3>Debugging</h3>
        
        <h3>Distribution Techniques</h3>
        
        <h3>CUDA Kernels</h3>
        
        <h3>Hardware</h3>
        
        <h3>Others</h3>
        
        <h2>Appendix</h2>

    </d-article>

    <d-appendix>
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <style>
            d-appendix .citation {
                font-size: 11px;
                line-height: 15px;
                border-left: 1px solid rgba(0, 0, 0, 0.1);
                padding-left: 18px;
                border: 1px solid rgba(0, 0, 0, 0.1);
                background: rgba(0, 0, 0, 0.02);
                padding: 10px 18px;
                border-radius: 3px;
                color: rgba(150, 150, 150, 1);
                overflow: hidden;
                margin-top: -12px;
                white-space: pre-wrap;
                word-wrap: break-word;
            }
        </style>

        <h3 id="citation">Citation</h3>
        <p>For attribution in academic contexts, please cite this work as</p>
        <pre
            class="citation short">XXX, et al., "The Ultra-Scale Playbook:¬†Training LLMs on GPU Clusterse", 2025.</pre>
        <p>BibTeX citation</p>
        <pre class="citation long">@misc{TODO,
      title={The Ultra-Scale Playbook:¬†Training LLMs on GPU Clusters},
      author={TODO},
      year={2025},
}</pre>
    </d-appendix>

    <script>
        const article = document.querySelector('d-article');
        const toc = document.querySelector('d-contents');
        if (toc) {
            const headings = article.querySelectorAll('h2, h3, h4');
            let ToC = `<nav role="navigation" class="l-text figcaption"><h3>Table of contents</h3>`;
            let prevLevel = 0;

            for (const el of headings) {
                // should element be included in TOC?
                const isInTitle = el.parentElement.tagName == 'D-TITLE';
                const isException = el.getAttribute('no-toc');
                if (isInTitle || isException) continue;
                el.setAttribute('id', el.textContent.toLowerCase().replaceAll(" ", "_"))
                const link = '<a target="_self" href="' + '#' + el.getAttribute('id') + '">' + el.textContent + '</a>';

                const level = el.tagName === 'H2' ? 0 : (el.tagName === 'H3' ? 1 : 2);
                while (prevLevel < level) {
                    ToC += '<ul>'
                    prevLevel++;
                }
                while (prevLevel > level) {
                    ToC += '</ul>'
                    prevLevel--;
                }
                if (level === 0)
                    ToC += '<div>' + link + '</div>';
                else
                    ToC += '<li>' + link + '</li>';
            }

            while (prevLevel > 0) {
                ToC += '</ul>'
                prevLevel--;
            }
            ToC += '</nav>';
            toc.innerHTML = ToC;
            toc.setAttribute('prerendered', 'true');
            const toc_links = document.querySelectorAll('d-contents > nav a');

            window.addEventListener('scroll', (_event) => {
                if (typeof (headings) != 'undefined' && headings != null && typeof (toc_links) != 'undefined' && toc_links != null) {
                    // Then iterate forwards, on the first match highlight it and break
                    find_active: {
                        for (let i = headings.length - 1; i >= 0; i--) {
                            if (headings[i].getBoundingClientRect().top - 50 <= 0) {
                                if (!toc_links[i].classList.contains("active")) {
                                    toc_links.forEach((link, _index) => {
                                        link.classList.remove("active");
                                    });
                                    toc_links[i].classList.add('active');
                                }
                                break find_active;
                            }
                        }
                        toc_links.forEach((link, _index) => {
                            link.classList.remove("active");
                        });
                    }
                }
            });
        }
    </script>

</body>

</html>