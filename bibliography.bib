@article{tenenbaum2000global,
  title={A global geometric framework for nonlinear dimensionality reduction},
  author={Tenenbaum, Joshua B and De Silva, Vin and Langford, John C},
  journal={Science},
  volume={290},
  number={5500},
  pages={2319--2323},
  year={2000},
  publisher={American Association for the Advancement of Science},
  url={https://science.sciencemag.org/content/290/5500/2319}
}

@article{roweis2000nonlinear,
  title={Nonlinear dimensionality reduction by locally linear embedding},
  author={Roweis, Sam T and Saul, Lawrence K},
  journal={Science},
  volume={290},
  number={5500},
  pages={2323--2326},
  year={2000},
  publisher={American Association for the Advancement of Science},
  url={https://science.sciencemag.org/content/290/5500/2323}
}

@article{belkin2003laplacian,
  title={Laplacian eigenmaps for dimensionality reduction and data representation},
  author={Belkin, Mikhail and Niyogi, Partha},
  journal={Neural computation},
  volume={15},
  number={6},
  pages={1373--1396},
  year={2003},
  publisher={MIT Press},
  url={https://direct.mit.edu/neco/article/15/6/1373/6699/Laplacian-Eigenmaps-for-Dimensionality-Reduction}
}

@misc{olah2014neural,
  title={Neural Networks, Manifolds, and Topology},
  author={Christopher Olah},
  year={2014},
  howpublished={Blog post},
  url={https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/},
  note={Accessed: 2025}
}

@article{power2022grokking,
  title={Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  author={Alethea Power and Yuri Burda and Harri Edwards and Igor Babuschkin and Vedant Misra},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022},
  url={https://arxiv.org/abs/2201.02177}
}

@article{park2024linear,
  title={The Linear Representation Hypothesis and the Geometry of Large Language Models},
  author={Kiho Park and Yo Joong Choe and Victor Veitch},
  journal={arXiv preprint arXiv:2311.03658},
  year={2024},
  url={https://arxiv.org/abs/2311.03658}
}

@article{trigonometry2025language,
  title={Language Models Use Trigonometry to Do Addition},
  author={Anonymous Authors},
  journal={arXiv preprint arXiv:2502.00873},
  year={2025},
  url={https://arxiv.org/abs/2502.00873}
}

@article{park2024geometry,
  title={The Geometry of Categorical and Hierarchical Concepts in Large Language Models},
  author={Kiho Park and Yo Joong Choe and Yibo Jiang and Victor Veitch},
  journal={arXiv preprint arXiv:2406.01506},
  year={2024},
  url={https://arxiv.org/abs/2406.01506}
}

@misc{lesswrong2024intricacies,
  title={Intricacies of Feature Geometry in Large Language Models},
  author={7vik and Lucius Bushnaq and Nandi Schoots},
  year={2024},
  howpublished={LessWrong},
  url={https://www.lesswrong.com/posts/eczwWrmX5XNEo7JsS/intricacies-of-feature-geometry-in-large-language-models},
  note={Accessed: 2025}
}

@article{engels2024not,
  title={Not All Language Model Features Are Linear},
  author={Joshua Engels and Isaac Liao and Eric J. Michaud and Wes Gurnee and Max Tegmark},
  journal={arXiv preprint arXiv:2405.14860},
  year={2024},
  url={https://arxiv.org/abs/2405.14860}
}

@misc{olah2024linear,
  title={What is a Linear Representation? What is a Multidimensional Feature?},
  author={Christopher Olah},
  year={2024},
  howpublished={Transformer Circuits Thread - July Update},
  url={https://transformer-circuits.pub/2024/july-update/index.html#linear-representations},
  note={Accessed: 2025}
}

@misc{johnson1984extensions,
  title={Extensions of Lipschitz mappings into a Hilbert space},
  author={William B. Johnson and Joram Lindenstrauss},
  year={1984},
  howpublished={Conference in modern analysis and probability},
  url={https://de.wikipedia.org/wiki/Lemma_von_Johnson-Lindenstrauss},
  note={Johnson-Lindenstrauss Lemma for near orthogonality in high dimensions}
}

@misc{3blue1brown2024high,
  title={If you generate random vectors in very very high dimensions, they are sort of nearly orthogonal},
  author={Grant Sanderson},
  year={2024},
  howpublished={3Blue1Brown YouTube video},
  url={https://www.youtube.com/watch?v=9-Jl0dxWQs8&t=3s},
  note={Accessed: 2025}
}

@article{tigges2023linear,
  title={Linear Representations of Sentiment in Large Language Models},
  author={Curt Tigges and Oskar Hollinsworth and Atticus Geiger and Neel Nanda},
  journal={arXiv preprint arXiv:2310.15154},
  year={2023},
  url={https://arxiv.org/abs/2310.15154}
}

@article{jiang2024origins,
  title={On the Origins of Linear Representations in Large Language Models},
  author={Yibo Jiang and Goutham Rajendran and Pradeep Kumar Ravikumar and Bryon Aragam and Victor Veitch},
  journal={Proceedings of the 41st International Conference on Machine Learning},
  year={2024},
  url={https://dl.acm.org/doi/10.5555/3692070.3692949}
}

@misc{yoder2025johnson,
  title={Beyond Orthogonality: How Language Models Pack Billions of Concepts into 12,000 Dimensions},
  author={Nick Yoder},
  year={2025},
  howpublished={Blog post},
  url={https://nickyoder.com/johnson-lindenstrauss/},
  note={Accessed: 2025}
}

@article{robinson2024token,
  title={Token embeddings violate the manifold hypothesis},
  author={Michael Robinson and Sourya Dey and Tony Chiang},
  journal={arXiv preprint arXiv:2504.01002},
  year={2024},
  url={https://arxiv.org/abs/2504.01002}
}

@article{li2025unraveling,
  title={Unraveling the Localized Latents: Learning Stratified Manifold Structures in LLM Embedding Space with Sparse Mixture-of-Experts},
  author={Xin Li and Anand D. Sarwate},
  journal={arXiv preprint arXiv:2502.13577},
  year={2025},
  url={https://arxiv.org/abs/2502.13577}
}

@inproceedings{ravfogel2020null,
  title={Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection},
  author={Shauli Ravfogel and Yanai Elazar and Hila Gonen and Michael Twiton and Yoav Goldberg},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7237--7256},
  year={2020},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/2020.acl-main.647/}
}

@misc{millidge2022singular,
  title={The Singular Value Decompositions of Transformer Weight Matrices are Highly Interpretable},
  author={Beren Millidge and Sid Black},
  year={2022},
  howpublished={LessWrong},
  url={https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight},
  note={Accessed: 2025}
}

@article{makelov2023subspace,
  title={Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching},
  author={Aleksandar Makelov and Georg Lange and Neel Nanda},
  journal={arXiv preprint arXiv:2311.17030},
  year={2023},
  url={https://arxiv.org/abs/2311.17030}
}

@article{heimersheim2024activation,
  title={How to use and interpret activation patching},
  author={Stefan Heimersheim and Neel Nanda},
  journal={arXiv preprint arXiv:2404.15255},
  year={2024},
  url={https://arxiv.org/abs/2404.15255}
}

@article{friedman2024interpretability,
  title={Interpretability Illusions in the Generalization of Simplified Models},
  author={Dan Friedman and Andrew Kyle Lampinen and Lucas Dixon and Danqi Chen and Asma Ghandeharioun},
  journal={International Conference on Machine Learning (ICML)},
  year={2024},
  url={https://arxiv.org/abs/2312.03656}
}

@misc{hase2023localization,
  title={Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models},
  author={Peter Hase and Mohit Bansal and Been Kim and Asma Ghandeharioun},
  year={2023},
  howpublished={Advances in Neural Information Processing Systems (NeurIPS)},
  note={Spotlight paper}
}

@misc{white2023nonlinear,
  title={A Non-Linear Structural Probe},
  author={Jennifer C. White and Ryan Cotterell},
  year={2023},
  howpublished={arXiv preprint},
  note={Paper on non-linear probing methods for understanding linguistic structure in neural networks}
}

@misc{shapley2023knowing,
  title={Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data},
  author={Authors TBD},
  year={2023},
  howpublished={Research paper},
  note={Paper on using Shapley interactions to understand nonlinear data structures}
}

@misc{numerical2024geometry,
  title={The Geometry of Numerical Reasoning: Language Models Compare Numeric Properties in Linear Subspaces},
  author={Authors TBD},
  year={2024},
  howpublished={Research paper},
  note={Paper on how language models represent and process numerical information geometrically}
}

@misc{lampinen2024learned,
  title={Learned feature representations are biased by complexity, learning order, position, and more},
  author={Andrew K. Lampinen and Stephanie C. Y. Chan and Katja M. Hermann},
  journal={arXiv preprint arXiv:2405.05847},
  year={2024},
  url={https://arxiv.org/abs/2405.05847}
}